{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 基本函数学习笔记一\n",
    "\n",
    "spark的函数主要分两类，Transformations和Actions。\n",
    "\n",
    "Transformations为一些数据转换类函数，actions为一些行动类函数：\n",
    "\n",
    "- 转换：转换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，  \n",
    "    不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。\n",
    "- 行动：行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，  \n",
    "    会在这一时刻计算全部的数据处理查询并返回结果值。\n",
    "\n",
    "\n",
    "这里介绍pyspark中常用函数功能以及代码示例。\n",
    "\n",
    "官方文档链接：http://spark.apache.org/docs/2.3.3/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "RDD下面的Transformations函数，这些函数适用于RDD集合操作：\n",
    "\n",
    "- map(func)\n",
    "- flatMap(func)\n",
    "- mapPartitions(func)\n",
    "- mapPartitionsWithIndex(func)\n",
    "- foreach(f)\n",
    "- foreachPartition(f)\n",
    "- filter(func)\n",
    "- sample()\n",
    "- union()\n",
    "- intersection()\n",
    "- distinct()\n",
    "- groupBy()\n",
    "- groupByKey()\n",
    "- reduce\n",
    "- reduceByKey()\n",
    "- aggregate\n",
    "- aggregateByKey()\n",
    "- sortBy\n",
    "- sortByKey()\n",
    "- join()\n",
    "- cogroup()\n",
    "- cartesian()\n",
    "- coalesce()\n",
    "- Pipe()\n",
    "- Repartition()\n",
    "- rePartitionAndSortWithinPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.Builder().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map(func)转换\n",
    "\n",
    "map(func) 与python的map函数功能一样，都是对每个元素执行func函数的计算。\n",
    "\n",
    "返回一个新的数据集，数据集的每个元素都是经过func函数处理的\n",
    "\n",
    "我们这里以对每个元素乘以10计算示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用lambda 函数\n",
    "temp = rdd.map(lambda x: x*10)\n",
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义函数\n",
    "def multi_10(x):\n",
    "    return x * 10\n",
    "\n",
    "temp = rdd.map(multi_10)\n",
    "temp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap(func)\n",
    "\n",
    "类似于map(func)， 但是不同的是map对每个元素处理完后返回与原数据集相同元素数量的数据集，而flatMap返回的元素数不一定和原数据集相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 3, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([[1,2],[2,3],[3,4]])\n",
    "\n",
    "d = rdd.flatMap(lambda x: x)\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapPartitions(func)\n",
    "\n",
    "mapPartitions是map的一个变种。\n",
    "\n",
    "map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，\n",
    "\n",
    "也就是把每个分区中的内容作为整体来处理的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5], 3)\n",
    "def f(iterator): \n",
    "    yield sum(iterator)\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glom()函数就是要显示出RDD对象的分区情况\n",
    "\n",
    "可以看到rdd分了三个区，每个区的数据为： [[1], [2, 3], [4, 5]]\n",
    "\n",
    "所以上面的例子中mapPartitions计算sum的结果是三个，\n",
    "\n",
    "每个分区求和结果是[1,5,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapPartitionsWithIndex(func)\n",
    "与mapPartition相比，mapPartitionWithIndex能够保留分区索引，函数的传入参数也是分区索引和iterator构成的键值对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)], [(1, 5)], [(2, 9)]]\n",
      "[[1], [5], [9]]\n"
     ]
    }
   ],
   "source": [
    "def f1(partitionIndex,iterator):\n",
    "    yield (partitionIndex,sum(iterator))\n",
    "    \n",
    "def f2(partitionIndex,iterator):\n",
    "    yield sum(iterator)\n",
    "    \n",
    "rf1 = rdd.mapPartitionsWithIndex(f1)\n",
    "rf2 = rdd.mapPartitionsWithIndex(f2)\n",
    "\n",
    "# f1 的返回值可以保留分区索引\n",
    "print(rf1.glom().collect())\n",
    "print(rf2.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foreach(func)\n",
    "\n",
    "在RDD上每个元素执行func运算，foreach与map的区别是：\n",
    "\n",
    "**map返回一个新的RDD，foreach直接在元素上应用func操作，**\n",
    "\n",
    "**原RDD内容不变，无返回值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "res = rdd.foreach(lambda x: x*2)\n",
    "print(res)  # 打印结果为None\n",
    "\n",
    "rdd.collect() # 输出为 [1, 2, 3, 4, 5]\n",
    "\n",
    "# 打印元素，如果使用jupyter，在启动页面可以看到打印\n",
    "def f(x): \n",
    "    print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foreachPartition(f)\n",
    "\n",
    "与foreach一样，只是操作的对象为RDD的每个数据分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "        print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter(func)函数\n",
    "\n",
    "返回一个新的数据集，这个数据集中的元素是通过func函数筛选后返回为true的元素\n",
    "\n",
    "简单的说就是，对数据集中的每个元素进行筛选，如果符合条件则返回true，不符合返回false，\n",
    "\n",
    "最后将返回为true的元素组成新的数据集返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择偶数\n",
    "d = rdd.filter(lambda x: x % 2 ==0)\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 整除3\n",
    "def three(x):\n",
    "    return x % 3 == 0\n",
    "\n",
    "d = rdd.filter(three)\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample()\n",
    "\n",
    "sample()方法返回数据集的随机样本。\n",
    "- 第一个参数指定采样是否应该替换\n",
    "- 第二个参数定义返回数据的分数\n",
    "- 第三个参数是伪随机数产生器的种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(False, 0.4, 40).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# union\n",
    "\n",
    "union(otherDataset)是数据合并，返回一个新的数据集，由原数据集和otherDataset联合而成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda x: x + 10)\n",
    "rdd.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intersection()\n",
    "\n",
    "返回两个数据集的交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distinct()\n",
    "\n",
    "返回数据集中不同值的列表，即去除数据集中重复的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,1,2,3])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupBy\n",
    "\n",
    "给定一个分组条件，返回分组后的key value数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 2, 4, 6, 8]), (1, [1, 3, 5, 7, 9])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "# 按照模2结果来分组\n",
    "res = rdd.groupBy(lambda x: x % 2).collect()\n",
    "[(k, sorted(v)) for k,v in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 3, 6, 9]), (1, [1, 4, 7]), (2, [2, 5, 8])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照模3结果来分组\n",
    "res = rdd.groupBy(lambda x: x % 3).collect()\n",
    "[(k, sorted(v)) for k,v in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupByKey\n",
    "\n",
    "数据按照key来分区，数据要求为key value格式。\n",
    "\n",
    "返回的为key value格式，value为分组后的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1),\n",
       " ('a', 1),\n",
       " ('b', 1),\n",
       " ('b', 1),\n",
       " ('b', 1),\n",
       " ('c', 1),\n",
       " ('c', 1),\n",
       " ('c', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a', 1), ('a',1), ('b',1), ('b',1), ('b',1), ('c',1),('c',1),('c',1)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 3), ('c', 3), ('a', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分组后求值的和\n",
    "rdd.groupByKey().mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduce\n",
    "\n",
    "与python中的reduce功能一样，对iter对象的元素前两个元素进行函数操作，得到的结果与第三个元素再进行函数操作，\n",
    "\n",
    "以此类推，直到最后一个元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "# 累计求和\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduceByKey\n",
    "\n",
    "功能与reduce函数一样。不过输入的数据为key value格式，按照key分组进行函数操作。\n",
    "\n",
    "在WordCount例子中，使用reduceByKey来统计单词的个数。\n",
    "链接：https://www.cnblogs.com/StitchSun/p/10535822.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 3), ('c', 3), ('a', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a', 1), ('a',1), ('b',1), ('b',1), ('b',1), ('c',1),('c',1),('c',1)])\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate()\n",
    "\n",
    "函数原型：aggregate(zeroValue, seqOp, combOp)\n",
    "\n",
    "aggregate函数操作比较复杂，有两个函数。seqOp函数是对每个分区的元素与zoroValue进行计算，\n",
    "\n",
    "然后由combOp函数对所有分区的结果进行计算。\n",
    "\n",
    "\n",
    "将初始值和第一个分区中的第一个元素传递给seq函数进行计算，然后将计算结果和第二个元素传递给seq函数，直到计算到最后一个值。\n",
    "\n",
    "第二个分区中也是同理操作。\n",
    "\n",
    "最后将初始值、所有分区的结果经过combine函数进行计算（先将前两个结果进行计算，将返回结果和下一个结果传给combine函数，以此类推），并返回最终结果。\n",
    "\n",
    "http://www.cnblogs.com/LHWorldBlog/p/8215529.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n",
      "comOp:3\t1\n",
      "comOp:4\t3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize((1,2,3,4,5,6),2)\n",
    "\n",
    "# 如果使用jupyter，打印结果在jupyter页面可以看到\n",
    "def seq(a,b):\n",
    "    print('seqOp:'+str(a)+\"\\t\"+str(b))\n",
    "    return min(a,b)\n",
    "\n",
    "def combine(a,b):\n",
    "    print('comOp:'+str(a)+\"\\t\"+str(b))\n",
    "    return a+b\n",
    "\n",
    "# 例子解析：\n",
    "# 先在每个分区中元素中两两操作，找最小的元素。\n",
    "# 计算完成后，由combine函数计算两两分区结果的和\n",
    "# 计算步骤1：初始值为3，与分区[1,2,3]元素一一进行seq最小值运算，得到结果为 1\n",
    "# 计算步骤2：初始值为3，与分区[4,5,6]元素一一进行seq最小值运算，得到结果为 3\n",
    "# 计算步骤3：初始值为3，与分区结果1，分区结果3进行combine相加运算，得到结果为 3 + 1 + 3, 结果为7 \n",
    "print(data.glom().collect())\n",
    "data.aggregate(3,seq, combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregateByKey()\n",
    "\n",
    "函数原型\n",
    "aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)\n",
    "    \n",
    "在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，\n",
    "\n",
    "返回的结果作为一个新的kv对，然后再将结果按照key进行合并，\n",
    "\n",
    "最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），\n",
    "\n",
    "将key与计算结果作为一个新的kv对输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 3)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(1,3),(1,2),(1,4),(2,3)])\n",
    "\n",
    "def seqFunc(a, b):\n",
    "    print(\"seqFunc:%s,%s\" %(a,b))\n",
    "    return max(a, b) #取最大值\n",
    "\n",
    "def combFunc(a, b):\n",
    "    print(\"combFunc:%s,%s\" %(a ,b))\n",
    "    return a + b #累加起来\n",
    "\n",
    "# aggregateByKey这个算子内部有分组\n",
    "# 这里numPartitions值为4，将数据分为四个区，seqFunc计算结果为 （1，3），（1，3），（1，4）和（2，3）\n",
    "# 然后按照key分组进行comFunc计算，得到结果[(1, 10), (2,3)]\n",
    "data.aggregateByKey(3, seqFunc, combFunc, 4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sortBy\n",
    "sortBy(keyfunc, ascending=True, numPartitions=None)\n",
    "\n",
    "对集合元素排序，根据给定的函数进行排序操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"
     ]
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "\n",
    "x0 = sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
    "\n",
    "x1 = sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
    "\n",
    "print(x0)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sortByKey\n",
    "\n",
    "函数原型\n",
    "sortByKey(ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda>>)\n",
    "    \n",
    "根据key进行排序，输入的数据必须为key value格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join()\n",
    "\n",
    "两个数据集按照key内连接，返回数据集中有相同key的元素组成的新的数据集，\n",
    "\n",
    "数据集A中的元素与数据集B中相同key元素一一组合，生成新的数据集\n",
    "\n",
    "格式为(key, (value1, value2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "\n",
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cogroup()\n",
    "\n",
    "将多个RDD中同一个Key对应的Value组合到一起。如果RDD中没有对应的key，则会生成一个空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ([1], [2])\n",
      "b ([4], [])\n"
     ]
    }
   ],
   "source": [
    "for k, v in x.cogroup(y).collect():\n",
    "    print(k, tuple(map(list, v))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cartesian()\n",
    "\n",
    "笛卡尔集，是通过两个数据集计算而成的\n",
    "\n",
    "数据集a的每个元素与数据集b的每个元素组合，形成新的数据对数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2])\n",
    "rdd2 = sc.parallelize([3, 4, 5])\n",
    "\n",
    "rdd.cartesian(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coalesce()\n",
    "\n",
    "按照给定的数量重新分区数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old = sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
    "print(old)\n",
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3, 4, 5]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipe\n",
    "\n",
    "对rdd数据集的每个元素，调用外部程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for pyspark function test',\n",
       " 'for pyspark function test',\n",
       " 'for pyspark function test',\n",
       " 'for pyspark function test']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat 文件内容\n",
    "# 先在目录下创建一个for_test.txt文件，然后来读取文件内容\n",
    "sc.parallelize(['1', '2', '', '3']).pipe('cat for_test.txt').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repartition\n",
    "数据集重新分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5], [6, 7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7], []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建默认为四个分区的数据集\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "# 重新分为两个分区\n",
    "rdd.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rePartitionAndSortWithinPartitions()\n",
    "\n",
    "根据给定的分区程序对RDD进行重新分区，并在每个生成的分区内按键对记录进行排序。 \n",
    "\n",
    "这比调用重新分区，然后在每个分区内进行排序更有效率，因为它可以将排序压入洗牌机器。\n",
    "\n",
    "应用场景：\n",
    "- 如果需要重分区，并且想要对分区中的数据进行升序排序。\n",
    "- 提高性能，替换repartition和sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
