{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark初步-从wordcount开始\n",
    "\n",
    "spark中自带的example，有一个wordcount例子，我们逐步分析wordcount代码，开始我们的spark之旅。\n",
    "\n",
    "## 准备工作\n",
    "\n",
    "把README.md文件复制到当前的文件目录，启动jupyter，编写我们的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化spark实例，并把应用命名为wordcount\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件读取内容\n",
    "# 此时data为dataframe格式，每一行为文件中的一行\n",
    "data = spark.read.text(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# Apache Spark')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看第一行数据\n",
    "f = data.first()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Apache Spark'),\n",
       " Row(value=''),\n",
       " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
       " Row(value='high-level APIs in Scala, Java, Python, and R, and an optimized engine that'),\n",
       " Row(value='supports general computation graphs for data analysis. It also supports a')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看前5行数据\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把数据转换为rdd格式，并取出值\n",
    "data2 = data.rdd.map(lambda x: x[0])\n",
    "# 查看第一行数据，可以看到数据为string格式\n",
    "data2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'Apache', 'Spark', '', 'Spark']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于每行按照空格来分割，并把结果拉平\n",
    "data3 = data2.flatMap(lambda x: x.split(' '))\n",
    "# 查看前5个数据，可以看到已经分割为单个词了\n",
    "data3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 1), ('', 1), ('Spark', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为每个单词标记次数1\n",
    "data4 = data3.map(lambda x: (x,1))\n",
    "# 结果为turple类型，前面是key，后面的数字为单词的次数\n",
    "data4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 16),\n",
       " ('', 71),\n",
       " ('is', 6),\n",
       " ('a', 8),\n",
       " ('fast', 1),\n",
       " ('and', 9),\n",
       " ('general', 3),\n",
       " ('cluster', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 汇总统计每个单词出现的次数\n",
    "data5 = data4.reduceByKey(add)\n",
    "# 结果为turple类型，数字为单词的出现次数\n",
    "data5.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 71), ('the', 24), ('to', 17), ('Spark', 16), ('for', 12)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照出现次数多少来排序\n",
    "res = data5.sortBy(lambda x: x[1], ascending=False).collect()\n",
    "res[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 71), ('the', 24), ('to', 17), ('Spark', 16), ('for', 12), ('and', 9), ('##', 9), ('a', 8), ('can', 7), ('on', 7)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "data = spark.read.text(\"README.md\")\n",
    "\n",
    "data1 = data.rdd.map(lambda x: x[0])\n",
    "\n",
    "data2 = data1.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "\n",
    "res = data2.sortBy(lambda x: x[1], ascending=False).collect()\n",
    "\n",
    "print(res[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
